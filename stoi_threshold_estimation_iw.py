# -*- coding: utf-8 -*-
"""Stoi_Threshold_Estimation_IW.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1YvypYyrK6M8wFXVrEI7cgHPF4KKkY9Tg
"""

!pip install -q transformers datasets
!pip install -q jiwer

!pip install -q noisereduce
!pip install -q -U denoiser

import os
import torch
from transformers import pipeline
import pandas as pd
from jiwer import wer
import librosa
import logging
from tqdm import tqdm
from denoiser import pretrained
from denoiser.dsp import convert_audio
import noisereduce as nr
logging.basicConfig(level=logging.INFO)

from google.colab import drive

# Mount Google Drive without user interaction
drive.mount('/content/drive', force_remount=True)

from scipy.signal import butter, filtfilt

def line_enhancer(data, sample_rate):
    # Define high-pass filter parameters
    cutoff_frequency = 300  # Cut-off frequency for the high-pass filter
    nyquist_frequency = 0.5 * sample_rate
    normalized_cutoff_frequency = cutoff_frequency / nyquist_frequency

    # Create a high-pass filter
    b, a = butter(4, normalized_cutoff_frequency, btype='high', analog=False)

    # Convert data to a C-contiguous array
    data = np.array(data, dtype=np.float32, order='C')
    data = data.copy()
    #print(data)
    # Make a copy of the array to ensur data = data.copy()e C-contiguous memory layout
    # Apply the high-pass filter to the copied data
    enhanced_data = filtfilt(b, a, data)

    return enhanced_data.copy()

def sg_noisereduce(data, sample_rate):
    # Define high-pass filter parameters
  reduced_noise = nr.reduce_noise(y=data, sr=sample_rate)

  return reduced_noise

model_denoiser = pretrained.dns64().cuda()

def dns64_denoise_waveform(audio):
  audio = torch.from_numpy(audio).reshape(1, -1)
  wav = convert_audio(audio.cuda(), 16000, model_denoiser.sample_rate, model_denoiser.chin)
  with torch.no_grad():
    denoised = model_denoiser(wav[None])[0]
  return denoised.data.cpu().numpy().reshape(-1)

def denoised_audio(audio, denoising_type):

  if denoising_type == "dm":
    audio = dns64_denoise_waveform(audio)
    audio = audio.copy()
  elif denoising_type == "le":
    audio = line_enhancer(audio, 16000)
    audio = audio.astype(np.float32)
    audio = audio.copy()
  elif denoising_type == "dm_le":
    audio = dns64_denoise_waveform(audio)
    audio = audio.copy()
    audio = line_enhancer(audio, 16000)
    audio = audio.astype(np.float32)
    audio = audio.copy()
  elif denoising_type == "le_dm":
    audio = line_enhancer(audio, 16000)
    audio = audio.astype(np.float32)
    audio = audio.copy()
    audio = dns64_denoise_waveform(audio)
    audio = audio.copy()
  elif denoising_type == "sg":
    audio = sg_noisereduce(audio, 16000)
    audio = audio.copy()
  elif denoising_type == "baseline":
    audio = audio


  return audio

import os
import random
import torch
import torchaudio
import numpy as np
import pandas as pd

from jiwer import wer, compute_measures

from IPython.display import Audio as audio_show
#from datasets import load_dataset, Audio

from transformers import Wav2Vec2ForCTC, AutoProcessor

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# model_id = "facebook/mms-1b-all"
# # model_id = "facebook/mms-300m"

# processor = AutoProcessor.from_pretrained(model_id)
# model = Wav2Vec2ForCTC.from_pretrained(model_id).to(device)

def load_model(model_path, device="cuda"):
    whisper_asr = pipeline("automatic-speech-recognition", model=model_path, device=device)
    return whisper_asr
def configure_model_for_language(whisper_asr, lang_code):
    if lang_code == 'or':
        whisper_asr.model.config.forced_decoder_ids = (
            whisper_asr.tokenizer.get_decoder_prompt_ids(
                language=None, task="transcribe"
            )
        )
    else:
        whisper_asr.model.config.forced_decoder_ids = (
            whisper_asr.tokenizer.get_decoder_prompt_ids(
                language=lang_code, task="transcribe"
            )
        )
    return whisper_asr

lang_code_map = {
    "af":"afr",  # Afrikaans
    "ar": "ara",  # Arabic
    "bn": "ben",  # Bengali
    "gu": "guj",  # Gujarati
    "hi": "hin",  # Hindi
    "li": "lit",  # Lithuanian
    "lv": "lav",  # Latvian
    "mr": "mar",  # Marathi
    "mi": "mri",  # Maori
    "or": "ory",  # Odia (formerly known as Oriya)
    "sw": "swh",  # Swahili
    "ta": "tam",  # Tamil
    "tl": "tel",  # Telugu
    "th": "tha",  # Thai
    "ur": "urd-script_arabic",  # Urdu
    "vi": "vie"   # Vietnamese
}

import logging
def transcribe_audio(whisper_asr, audio_dir, filename):
    try:
        logging.info(f"Processing: {filename}")
        audio_file = os.path.join(audio_dir, filename)
        audio, sr = librosa.load(audio_file, sr=16000)  # Load and resample to 16000 Hz
        result = whisper_asr(audio)
        transcription = result["text"]
        logging.info(f"Transcription: {transcription}")
        filename = filename.split("/")[-1]
        return {"ID": filename, "transcription": transcription}
    except Exception as e:
        logging.error(f"Error processing file {filename}: {e}")
        return None

import librosa
indian_lang_code_map = {
    "hi": "Hindi",
    "bn": "Bengali",
    "gu": "Gujarati",
    "mr": "Marathi",
    "or": "Odia",
    "ta": "Tamil",
    "te": "Telugu",
    "ur": "Urdu"
}

def get_wer(transcript, reference):
  measures = compute_measures(reference, transcript)
  total_reference_words = len(reference.split())

  wer = measures['wer']
  components = [
      wer
  ]

  return components


def preprocessing_on_audio(audio):
  # to be filled according to experiments
  preprocessed_audio = denoised_audio(audio, "dm")
  return preprocessed_audio


def get_transcription(audio_file, lang_code, preprocessing=False):
    # Load model and processor (you might want to do this outside the function for efficiency)
    if lang_code=='tl':
      lang_code='te'
    language=indian_lang_code_map.get(lang_code,0)
    if language == 0:
      return ""
    lower_language=language.lower()
    model_dir=f"/content/drive/MyDrive/WHISPER_MODELS/{lower_language}_models/whisper-medium-{lang_code}_alldata_multigpu"
    whisper_asr = load_model(model_dir)
    whisper_asr = configure_model_for_language(whisper_asr, lang_code)
    #whisper_asr = configure_model_for_language(whisper_asr, 'en')

    # Load and preprocess audio
    audio, _ = librosa.load(audio_file,sr = 16000)
    if preprocessing:
        audio = preprocessing_on_audio(audio)
    result = whisper_asr(audio)
    transcription = result["text"]

    return transcription


def get_transcription_and_wer(audio_file, reference, lang, preprocessing=True):
  # returns [transcription, wer_components .. ]

  transcription = [get_transcription(audio_file,lang, preprocessing)]
  wers = get_wer(transcription[0], reference)
  return transcription + wers

"""NOW IT HANDLES NAN ROWS CHANGE IT BEFORE RUN"""

import pandas as pd
#ref_csv = "/content/drive/MyDrive/RESULTS_1/INDIC_WHISPER_RESULTS/multilingual_test_all_stoi_except_vistaar.csv"
ref_csv="/content/drive/MyDrive/RESULTS_1/INDIC_WHISPER_RESULTS/multilingual_test_all_stoi_except_vistaar_test_IW_temp_dm.csv"
df = pd.read_csv(ref_csv)
print(df.shape)

model_type="IW"
source_dir ="/content/drive/MyDrive/PHD/"
output_dir='/content/drive/MyDrive/INTERN_RESULTS'
output1_dir='/content/drive/MyDrive/RESULTS_1/INDIC_WHISPER_RESULTS'
output_csv=os.path.join(output1_dir,f'multilingual_test_all_stoi_except_vistaar_test_{model_type}_dm.csv')

print(df.shape)

"""CODE FOR HANDLING PAUSED CSV IN THE MIDDLE"""

import os
import pandas as pd
import numpy as np
audio_dir='/content/drive/MyDrive'


indian_lang_codes_map = {
    "hi": "Hindi",
    "bn": "Bengali",
    "gu": "Gujarati",
    "mr": "Marathi",
    "or": "Odia",
    "ta": "Tamil",
    "tl": "Telugu",
    "ur": "Urdu"
}

# Function to process a single row
def process_row(row):
    try:
        file_name = os.path.join(audio_dir, row['actual_path'])
        lang_code=row["Language code"]
        if lang_code=='tl':
          lang_code='te'
        result = get_transcription_and_wer(file_name, row["Transcript"], lang_code)
        return result[0], result[1]  # Return transcript and WER
    except Exception as e:
        print(f"Error processing file {row['actual_path']}: {e}")
        return "", np.nan  # Return empty string for transcript and NaN for WER
# Filter out rows where 'IW_transcript_denoised' is empty
filtered_df = df[df['Language code'].isin(indian_lang_codes_map)]
print(filtered_df.shape)

# Find non-empty rows where 'IW_transcript_denoised' is empty
non_empty_rows = filtered_df[filtered_df['wer_IW_denoised'].isna()]
print("HAI")
print(non_empty_rows.shape)
c=0
# Iterate over non-empty rows to find denoised transcript and WER
for index, row in non_empty_rows.iterrows():
    print(index,row['ID'])
    lang_code = row['Language code']
    if lang_code == 'tl':
        lang_code = 'te'
    language = indian_lang_code_map.get(lang_code, 0)
    if language == 0:
        continue
    capital_language = language.upper()
    noised_csv_path = f"WHISPER_LARGE/INDIC_WHISPER_FINAL_RESULTS_1/{capital_language}_INDIC_WHISPER.xlsx"
    noised_csv = os.path.join(output_dir, noised_csv_path)
    if os.path.exists(noised_csv):
        noised_df = pd.read_excel(noised_csv)

        # Find the matching row in noised_df based on 'ID'
        id_value = row['ID']
        matching_row = noised_df[noised_df['path'] == id_value]
        noised_transcription = ""
        noised_wer = np.nan

        if not matching_row.empty:
            noised_transcription = matching_row.iloc[0]['transcripition']  # Fixed column name
            noised_wer = matching_row.iloc[0]['WER']
        else:
            print(f"No matching entry found for ID {id_value} in {noised_csv}")

        transcript, wer_value = process_row(row)

        # Update filtered_df based on the matching 'ID'
        filtered_df.loc[filtered_df['ID'] == id_value, f'{model_type}_transcript_denoised'] = transcript
        filtered_df.loc[filtered_df['ID'] == id_value, f'wer_{model_type}_denoised'] = wer_value
        filtered_df.loc[filtered_df['ID'] == id_value, f'{model_type}_transcript_noised'] = noised_transcription
        filtered_df.loc[filtered_df['ID'] == id_value, f'wer_{model_type}_noised'] = noised_wer
    # Save the DataFrame every 10 iterations
    if (index + 1) % 200 == 0:
      filtered_df.to_csv(output_csv, index=False)
      print(f"Saved DataFrame up to index {index + 1}")

# Save the DataFrame after processing all rows
filtered_df.to_csv(output_csv, index=False)
print("Finished processing and saved the grouped DataFrame.")

# Print the dimensions of the filtered DataFrame
print("Dimensions of filtered DataFrame:")
print(f"Rows: {df.shape[0]}")
print(f"Columns: {df.shape[1]}")

# Create new columns for the transcript and WE+
import os
import pandas as pd
import numpy as np
audio_dir='/content/drive/MyDrive'

# Create new columns for the transcript and WER
df[f'{model_type}_transcript_denoised'] = ''
df[f'wer_{model_type}_denoised'] = ''
df[f'{model_type}_transcript_noised'] = ''
df[f'wer_{model_type}_noised'] = ''

indian_lang_codes_map = {
    "hi": "Hindi",
    "bn": "Bengali",
    "gu": "Gujarati",
    "mr": "Marathi",
    "or": "Odia",
    "ta": "Tamil",
    "tl": "Telugu",
    "ur": "Urdu"
}
df[f'{model_type}_transcript_denoised'] = ''
df[f'wer_{model_type}_denoised'] = ''
df[f'{model_type}_transcript_noised'] = ''
df[f'wer_{model_type}_noised'] = ''

# Function to process a single row
def process_row(row):
  try:
    file_name = source_dir + row['actual_path']
    lang_code=row["Language code"]
    if lang_code=='tl':
      lang_code='te'
    result = get_transcription_and_wer(file_name, row["Transcript"], lang_code)
    return result[0], result[1]   # Return transcript and WER
  except Exception as e:
    print(f"Error processing {row['actual_path']}: {e}")
    return [""] + [np.nan] * 1   # Return empty string for transcript and NaN for WER

# Apply the function to each row in the DataFrame
for index, row in df.iterrows():
    lang_code = row['Language code']  # Replace 'lang_code' with the actual column name in your dataframe
    language = indian_lang_code_map.get(lang_code, None)

    if language is None:
        continue

    capital_language = language.upper()
    noised_csv_path = f"WHISPER_LARGE/INDIC_WHISPER_FINAL_RESULTS/{capital_language}_INDIC_WHISPER.csv"
    noised_csv = os.path.join(output_dir, noised_csv_path)

    if os.path.exists(noised_csv):
        noised_df = pd.read_csv(noised_csv)

        # Assuming 'ID' is the column name in both df and noised_df
        id_value = row['ID']  # Replace 'ID' with the actual column name in your dataframe
        matching_row = noised_df[noised_df['path'] == id_value]

        if not matching_row.empty:
            noised_transcription = matching_row.iloc[0]['noised_transcription']
            noised_wer = matching_row.iloc[0]['noised_wer']

            transcript, wer_value = process_row(row)

            df.loc[index, f'{model_type}_transcript_denoised'] = transcript
            df.loc[index, f'wer_{model_type}_denoised'] = wer_value

            df.loc[index, f'{model_type}_transcript_noised'] = noised_transcription
            df.loc[index, f'wer_{model_type}_noised'] = noised_wer



    # Save the DataFrame every 500 iterations
    if (index + 1) % 10 == 0:
      df.to_csv(output_csv, index=False)
      print(f"Saved DataFrame up to index {index + 1}")
      break;

# Save the final DataFrame
df.to_csv(output_csv, index=False)
print("Finished processing and saved the DataFrame.")

ref_df = pd.read_csv("/content/drive/MyDrive/PHD/MultiLingual/merged_mms_results.csv")

ref_df.head()

df.head()

# prompt: I want to generate the above statistics language code wise awg ,rwg and p_value for gender - i mean there has to be 5 columns language_code , median, awg corresponding to gender, rwg corresponding to gender and p-value for gender

df_stats = df # reference to dataframe
categories = ['Gender'] # list of all categories
languages = df_stats['Language code'].unique()

result_data = []

for lang in languages:
  lang_df = df_stats[df_stats['Language code'] == lang]
  print(lang, lang_df["wer_mms"].median())
  statistics = {}
  AWG = {}
  RWG = {}

  for category in categories:
    category_grp = lang_df.groupby(category)
    classes_cur_group = list(category_grp.groups.keys())
    temp = []

    for cls in classes_cur_group:
      temp.append(lang_df[lang_df[category] == cls]['wer_mms'])

    statistics[category] = stats.kruskal(*temp)
    AWG[category] = category_grp['wer_mms'].median().max() - category_grp['wer_mms'].median().min()
    RWG[category] = (AWG[category] / category_grp['wer_mms'].median().max()) * 100

  median_wer = lang_df["wer_mms"].median()
  p_value = statistics['Gender'].pvalue
  awg = AWG['Gender']
  rwg = RWG['Gender']

  result_data.append([lang, median_wer, awg, rwg, p_value])

result_df = pd.DataFrame(result_data, columns=['language_code', 'median', 'awg_gender', 'rwg_gender', 'p_value_gender'])
print(result_df)

## ALL DENOISED RESULTS

df_stats = df # reference to dataframe
categories = ['Gender'] # list of all categories
languages = df_stats['Language code'].unique()

result_data = []

for lang in languages:
  lang_df = df_stats[df_stats['Language code'] == lang]
  print(lang, lang_df["wer_mms_denoised"].median())
  statistics = {}
  AWG = {}
  RWG = {}

  for category in categories:
    category_grp = lang_df.groupby(category)
    classes_cur_group = list(category_grp.groups.keys())
    temp = []

    for cls in classes_cur_group:
      temp.append(lang_df[lang_df[category] == cls]['wer_mms_denoised'])

    statistics[category] = stats.kruskal(*temp)
    AWG[category] = category_grp['wer_mms_denoised'].median().max() - category_grp['wer_mms_denoised'].median().min()
    RWG[category] = (AWG[category] / category_grp['wer_mms_denoised'].median().max()) * 100

  median_wer = lang_df["wer_mms_denoised"].median()
  p_value = statistics['Gender'].pvalue
  awg = AWG['Gender']
  rwg = RWG['Gender']

  result_data.append([lang, median_wer, awg, rwg, p_value])

result_df = pd.DataFrame(result_data, columns=['language_code', 'median', 'awg_gender', 'rwg_gender', 'p_value_gender'])
print(result_df)

def get_result_according_to_stoi(row,threshold):
  if(row['stoi'] > threshold):
    return row['mms_transcript'], row['wer_mms']
  else:
    return row['mms_transcript_denoised'], row['wer_mms_denoised']

thresold_levels = [0.1,0.25,0.5,0.75,0.9]

### For MIN AWG


best_thresholds = {}

for lang in df['Language code'].unique():
  best_threshold = None
  best_median= None
  min_awg = float('inf')

  for threshold in thresold_levels:
    threshold_value = df[df['Language code'] == lang]["stoi"].quantile(threshold) # calculate percentile for each language
    df['mms_transcript_temp'], df['wer_mms_temp'] = zip(*df.apply(lambda row: get_result_according_to_stoi(row, threshold_value), axis=1))
    lang_df = df[df['Language code'] == lang]
    category_grp = lang_df.groupby('Gender')
    median_lang = lang_df["wer_mms_temp"].median()
    awg = category_grp['wer_mms_temp'].median().max() - category_grp['wer_mms_temp'].median().min()

    if awg < min_awg:
      min_awg = awg
      best_threshold = threshold_value
      best_median = median_lang

  # Handle the case where no best threshold was found (min_awg remains inf)
  if min_awg == float('inf'):
    print(f"Warning: No best threshold found for language {lang}. Using baseline results.")
    df.loc[df['Language code'] == lang, 'mms_transcript_best'] = df.loc[df['Language code'] == lang, 'mms_transcript']
    df.loc[df['Language code'] == lang, 'wer_mms_best'] = df.loc[df['Language code'] == lang, 'wer_mms']
  else:
    best_thresholds[lang] = (best_threshold, min_awg,median_lang)
    df.loc[df['Language code'] == lang, 'mms_transcript_best'] = df.loc[df['Language code'] == lang, 'mms_transcript_temp']
    df.loc[df['Language code'] == lang, 'wer_mms_best'] = df.loc[df['Language code'] == lang, 'wer_mms_temp']

df = df.drop(columns=['mms_transcript_temp', 'wer_mms_temp'])

for lang, (threshold, awg, median) in best_thresholds.items():
  print(f"Language: {lang}, Best Threshold: {threshold}, AWG for wer_mms_best: {awg},Median  for wer_mms_best: {median}")



### For MIN MEDIAN WER

best_thresholds = {}

for lang in df['Language code'].unique():
  best_threshold = None
  min_median_wer = float('inf')

  for threshold in thresold_levels:
    threshold_value = df[df['Language code'] == lang]["stoi"].quantile(threshold) # calculate percentile for each language

    df['mms_transcript_temp'], df['wer_mms_temp'] = zip(*df.apply(lambda row: get_result_according_to_stoi(row, threshold_value), axis=1))
    lang_df = df[df['Language code'] == lang]
    median_wer = lang_df["wer_mms_temp"].median()

    category_grp = lang_df.groupby('Gender')
    awg = category_grp['wer_mms_temp'].median().max() - category_grp['wer_mms_temp'].median().min()

    if median_wer < min_median_wer:
      min_median_wer = median_wer
      best_threshold = threshold
      best_awg = awg  # Store the corresponding AWG

  # Handle the case where no best threshold was found (min_median_wer remains inf)
  if min_median_wer == float('inf'):
    print(f"Warning: No best threshold found for language {lang}. Using baseline results.")
    df.loc[df['Language code'] == lang, 'mms_transcript_best'] = df.loc[df['Language code'] == lang, 'mms_transcript']
    df.loc[df['Language code'] == lang, 'wer_mms_best'] = df.loc[df['Language code'] == lang, 'wer_mms']
  else:
    best_thresholds[lang] = (best_threshold, min_median_wer, best_awg)
    df.loc[df['Language code'] == lang, 'mms_transcript_best'] = df.loc[df['Language code'] == lang, 'mms_transcript_temp']
    df.loc[df['Language code'] == lang, 'wer_mms_best'] = df.loc[df['Language code'] == lang, 'wer_mms_temp']

df = df.drop(columns=['mms_transcript_temp', 'wer_mms_temp'])

for lang, (threshold, median_wer, awg) in best_thresholds.items():
  print(f"Language: {lang}, Best Threshold: {threshold}, New Median WER: {median_wer}, New AWG: {awg}")

import pandas as pd
from scipy.stats import kruskal
import os

language_codes = {
    "hi": "Hindi",
    "bn": "Bengali",
    "gu": "Gujarati",
    "mr": "Marathi",
    "or": "Odia",
    "ta": "Tamil",
    "tl": "Telugu",
    "ur": "Urdu"
}

# Define threshold levels globally or pass as an argument
threshold_levels = [0.1, 0.25, 0.5, 0.75, 0.9]  # Example quantiles

def get_result_according_to_stoi(row, model_type, threshold):
    if row['stoi'] > threshold:
        return row[f'{model_type}_transcript_noised'], row[f'wer_{model_type}_noised']
    else:
        return row[f'{model_type}_transcript_denoised'], row[f'wer_{model_type}_denoised']

def calculate_awg_and_thresholds(input_file, output_file, model_type, quantile=0.1):
    # Load the CSV from the given location
    df = pd.read_csv(input_file)

    # Remove rows where either denoised or noised transcripts are empty
    df = df.dropna(subset=[f'{model_type}_transcript_denoised', f'{model_type}_transcript_noised'])

    # Initialize list to store results
    threshold_results = []

    # Process each language
    for lang in df['Language code'].unique():
        lang_df = df[df['Language code'] == lang]

        # Calculate the number of samples to consider (10%)
        sample_size = int(len(lang_df) * quantile)
        lang_df_sample = lang_df.sample(n=sample_size, random_state=42)

        # Initialize variables to find the best threshold
        best_threshold = None
        min_awg_median = float('inf')
        best_median = None

        for threshold in threshold_levels:
            threshold_value = lang_df_sample["stoi"].quantile(threshold)

            # Apply STOI-based logic to determine temporary WERs
            temp_results = lang_df_sample.apply(
                lambda row: get_result_according_to_stoi(row, model_type, threshold_value), axis=1)

            # Assign temp results safely
            lang_df_sample[f'{model_type}_transcript_temp'], lang_df_sample[f'wer_{model_type}_temp'] = zip(*temp_results)

            # Convert the WER column to numeric
            lang_df_sample[f'wer_{model_type}_temp'] = pd.to_numeric(lang_df_sample[f'wer_{model_type}_temp'], errors='coerce')

            # Calculate AWG median for current threshold
            category_grp = lang_df_sample.groupby('Gender')
            median_wer = lang_df_sample[f"wer_{model_type}_temp"].median()
            awg_median = category_grp[f'wer_{model_type}_temp'].median().max() - category_grp[f'wer_{model_type}_temp'].median().min()

            if awg_median < min_awg_median:
                min_awg_median = awg_median
                best_threshold = threshold
                best_median = median_wer

        # Store results for this language
        if best_threshold is not None:
            threshold_results.append({
                'Language': lang,
                'Threshold_Type': 'median',
                'model type': model_type,
                'threshold_value': lang_df_sample["stoi"].quantile(threshold),
                'Quantile': best_threshold,
                'Median_Corresponding_to_Quantile': best_median,
                'Best_AWG_Median': min_awg_median
            })

        # Remove sample data from the original DataFrame
        df = df.drop(lang_df_sample.index)

    threshold_df = pd.DataFrame(threshold_results)
    # Save results to CSV
    if not os.path.exists(output_file):
        threshold_df.to_csv(output_file, index=False)
    else:
        threshold_df.to_csv(output_file, mode='a', header=False, index=False)

    # Process the remaining 90% of the data
    remaining_results = process_remaining_data(df, threshold_df, model_type)

    return remaining_results

def process_remaining_data(df, threshold_df, model_type):
    results = []

    # Iterate over each language
    for lang in df['Language code'].unique():
        lang_df = df[df['Language code'] == lang].copy()  # Explicitly make a copy here
        threshold_row = threshold_df[(threshold_df['Language'] == lang) & (threshold_df['model type'] == model_type)]

        if not threshold_row.empty:
            threshold_value = threshold_row['Quantile'].values[0]
            quantile_threshold = lang_df['stoi'].quantile(threshold_value)

            # Select WER based on STOI values using .loc
            lang_df.loc[:, f'wer_{model_type}_final'] = lang_df.apply(
                lambda row: row[f'wer_{model_type}_noised'] if row['stoi'] > quantile_threshold else row[f'wer_{model_type}_denoised'],
                axis=1
            )

            # Convert the final WER column to numeric
            lang_df[f'wer_{model_type}_final'] = pd.to_numeric(lang_df[f'wer_{model_type}_final'], errors='coerce')

            # Calculate AWG median, AWG std, and p-value
            category_grp = lang_df.groupby('Gender')
            awg_median = category_grp[f'wer_{model_type}_final'].median().max() - category_grp[f'wer_{model_type}_final'].median().min()
            awg_std = category_grp[f'wer_{model_type}_final'].std().max() - category_grp[f'wer_{model_type}_final'].std().min()

            male_wer = lang_df[lang_df['Gender'] == 'M'][f'wer_{model_type}_final']
            female_wer = lang_df[lang_df['Gender'] == 'F'][f'wer_{model_type}_final']
            p_value = kruskal(male_wer, female_wer).pvalue

            results.append({
                'Language': language_codes[lang],
                'Language Code': lang,
                'AWG_Median': awg_median,
                'AWG_Std': awg_std,
                'PValue': p_value
            })

    return pd.DataFrame(results)


# Define input and output file paths
input_file = '/content/drive/MyDrive/RESULTS/multilingual_test_all_stoi_except_vistaar_test_IW.csv'
output_file_1 = '/content/drive/MyDrive/RESULTS/output_thresholds_IW.csv'
output_file_2 = '/content/drive/MyDrive/RESULTS/output_results_IW.csv'
model_type = 'IW'

# Call the function to calculate AWG and thresholds, and get remaining results
remaining_results_df = calculate_awg_and_thresholds(input_file, output_file_1, model_type)
remaining_results_df = remaining_results_df.sort_values(by='Language')

# Print or save the remaining results
print(remaining_results_df)
# Save the remaining results if needed
remaining_results_df.to_csv(output_file_2, index=False)

import pandas as pd
from scipy.stats import kruskal

# Define language codes for readability
language_codes = {
    "hi": "Hindi",
    "bn": "Bengali",
    "gu": "Gujarati",
    "mr": "Marathi",
    "or": "Odia",
    "ta": "Tamil",
    "tl": "Telugu",
    "ur": "Urdu"
}

def calculate_metrics(input_file, model_type='IW'):
    # Load the CSV file
    df = pd.read_csv(input_file)

    results = []

    # Iterate over each language
    for lang in df['Language code'].unique():
        lang_df = df[df['Language code'] == lang].copy()

        # Select the WER column based on the model_type
        wer_column = f'wer_{model_type}_denoised'
        lang_df[wer_column] = pd.to_numeric(lang_df[wer_column], errors='coerce')

        # Group by Gender and calculate AWG metrics
        category_grp = lang_df.groupby('Gender')
        awg_median = category_grp[wer_column].median().max() - category_grp[wer_column].median().min()
        awg_std = category_grp[wer_column].std().max() - category_grp[wer_column].std().min()

        # Calculate p-value using Kruskal-Wallis test
        male_wer = lang_df[lang_df['Gender'] == 'M'][wer_column].dropna()
        female_wer = lang_df[lang_df['Gender'] == 'F'][wer_column].dropna()
        p_value = kruskal(male_wer, female_wer).pvalue

        # Append results for this language
        results.append({
            'Language': language_codes.get(lang, lang),  # Fallback to code if name not found
            'Language Code': lang,
            'AWG_Median': awg_median,
            'AWG_Std': awg_std,
            'PValue': p_value
        })

    # Convert results to DataFrame
    results_df = pd.DataFrame(results)
    return results_df

# Define the input file path
input_file = '/content/drive/MyDrive/RESULTS/multilingual_test_all_stoi_except_vistaar_test_IW.csv'

# Call the function and get results
results_df = calculate_metrics(input_file, model_type='IW')

# Print or save the results
print(results_df)

# Optionally save the results to a CSV
results_df.to_csv('/content/drive/MyDrive/RESULTS/output_results_IW_DENOISED.csv', index=False)